{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os,sys\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datetime, time, json\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "'''\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print(train.isnull().sum())\n",
    "print(test.isnull().sum())\n",
    "\n",
    "# Preview some of the pairs of questions\n",
    "for i in range(6):\n",
    "    print(train.question1[i])\n",
    "    print(train.question2[i])\n",
    "    print()\n",
    "'''\n",
    "\n",
    "# Add the string 'empty' to empty strings\n",
    "train = train.fillna('empty')\n",
    "test = test.fillna('empty')\n",
    "\n",
    "\n",
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n",
    "    # Clean the text, with the option to remove stop_words and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    #text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\0k \", \"0000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "def process_questions(question_list, questions, question_list_name, dataframe):\n",
    "    '''transform questions and display progress'''\n",
    "    for question in questions:\n",
    "        question_list.append(text_to_wordlist(question))\n",
    "        if len(question_list) % 500000 == 0:\n",
    "            progress = len(question_list)/len(dataframe) * 100\n",
    "            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_question1 = []\n",
    "process_questions(train_question1, train.question1, 'train_question1', train)\n",
    "\n",
    "train_question2 = []\n",
    "process_questions(train_question2, train.question2, 'train_question2', train)\n",
    "\n",
    "test_question1 = []\n",
    "process_questions(test_question1, test.question1, 'test_question1', test)\n",
    "\n",
    "test_question2 = []\n",
    "process_questions(test_question2, test.question2, 'test_question2', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the length of questions\n",
    "lengths = []\n",
    "for question in train_question1:\n",
    "    lengths.append(len(question.split()))\n",
    "\n",
    "for question in train_question2:\n",
    "    lengths.append(len(question.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(lengths.counts.values, bins=100, kde=False)\n",
    "plt.xlabel('Sentence Length', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lengths.counts.describe()\n",
    "# print(np.percentile(lengths.counts, 99.0))\n",
    "# print(np.percentile(lengths.counts, 99.4))\n",
    "# print(np.percentile(lengths.counts, 99.5))\n",
    "# print(np.percentile(lengths.counts, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_question1 is complete.\n",
      "test_question2 is complete.\n",
      "Words in index: 120355\n"
     ]
    }
   ],
   "source": [
    "# tokenize the words for all of the questions\n",
    "all_questions = train_question1 + train_question2 + test_question1 + test_question2\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_questions)\n",
    "print(\"Fitting is complete.\")\n",
    "train_question1_word_sequences = tokenizer.texts_to_sequences(train_question1)\n",
    "print(\"train_question1 is complete.\")\n",
    "train_question2_word_sequences = tokenizer.texts_to_sequences(train_question2)\n",
    "print(\"train_question2 is complete\")\n",
    "\n",
    "test_question1_word_sequences = tokenizer.texts_to_sequences(test_question1)\n",
    "print(\"test_question1 is complete.\")\n",
    "test_question2_word_sequences = tokenizer.texts_to_sequences(test_question2)\n",
    "print(\"test_question2 is complete.\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_q1 is complete.\n",
      "train_q2 is complete.\n",
      "test_q1 is complete.\n",
      "test_q2 is complete.\n"
     ]
    }
   ],
   "source": [
    "# Pad the questions so that they all have the same length.\n",
    "max_question_len = 36\n",
    "train_q1 = pad_sequences(train_question1_word_sequences, maxlen = max_question_len)\n",
    "print(\"train_q1 is complete.\")\n",
    "\n",
    "train_q2 = pad_sequences(train_question2_word_sequences, maxlen = max_question_len)\n",
    "print(\"train_q2 is complete.\")\n",
    "\n",
    "test_q1 = pad_sequences(test_question1_word_sequences, \n",
    "                             maxlen = max_question_len,\n",
    "                             padding = 'post',\n",
    "                             truncating = 'post')\n",
    "print(\"test_q1 is complete.\")\n",
    "\n",
    "test_q2 = pad_sequences(test_question2_word_sequences, \n",
    "                             maxlen = max_question_len,\n",
    "                             padding = 'post',\n",
    "                             truncating = 'post')\n",
    "print(\"test_q2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = train.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 400000\n",
      "Null word embeddings: 43724\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "print('Word embeddings:', len(embeddings_index)) #151,250\n",
    "\n",
    "# Need to use 300 for embedding dimensions to match GloVe's vectors.\n",
    "embedding_dim = 300\n",
    "\n",
    "# Note for Kaggle users: Uncomment this too, because it relate to the code for GloVe.\n",
    "nb_words = len(word_index)\n",
    "word_embedding_matrix = np.zeros((nb_words + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "   embedding_vector = embeddings_index.get(word)\n",
    "   if embedding_vector is not None:\n",
    "       # words not found in embedding index will be all-zeros.\n",
    "       word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0)) #75,334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "units = 128 # Number of nodes in the Dense layers\n",
    "dropout = 0.5 # Percentage of nodes to drop\n",
    "nb_filter = 64 # Number of filters to use in Convolution1D\n",
    "filter_length = 7 # Length of filter for Convolution1D\n",
    "# Initialize weights and biases for the Dense layers\n",
    "# weights = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)\n",
    "weights = initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=2)\n",
    "bias = bias_initializer='zeros'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(nb_words + 1,\n",
    "                     embedding_dim,\n",
    "                     #weights = [word_embedding_matrix], Commented out for Kaggle\n",
    "                     input_length = max_question_len,\n",
    "                     trainable = False))\n",
    "\n",
    "model1.add(Convolution1D(filters = nb_filter, \n",
    "                         kernel_size = filter_length, \n",
    "                         padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(dropout))\n",
    "\n",
    "model1.add(Convolution1D(filters = nb_filter, \n",
    "                         kernel_size = filter_length, \n",
    "                         padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(dropout))\n",
    "\n",
    "model1.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(nb_words + 1,\n",
    "                     embedding_dim,\n",
    "                     #weights = [word_embedding_matrix],\n",
    "                     input_length = max_question_len,\n",
    "                     trainable = False))\n",
    "\n",
    "model2.add(Convolution1D(filters = nb_filter, \n",
    "                         kernel_size = filter_length, \n",
    "                         padding = 'same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(dropout))\n",
    "\n",
    "model2.add(Convolution1D(filters = nb_filter, \n",
    "                         kernel_size = filter_length, \n",
    "                         padding = 'same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(dropout))\n",
    "\n",
    "model2.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(nb_words + 1,\n",
    "                     embedding_dim,\n",
    "                     #weights = [word_embedding_matrix],\n",
    "                     input_length = max_question_len,\n",
    "                     trainable = False))\n",
    "model3.add(TimeDistributed(Dense(embedding_dim)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(dropout))\n",
    "model3.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(nb_words + 1,\n",
    "                     embedding_dim,\n",
    "                     #weights = [word_embedding_matrix],\n",
    "                     input_length = max_question_len,\n",
    "                     trainable = False))\n",
    "\n",
    "model4.add(TimeDistributed(Dense(embedding_dim)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dropout(dropout))\n",
    "model4.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\legacy\\layers.py:66: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  warnings.warn('The `Merge` layer is deprecated '\n"
     ]
    }
   ],
   "source": [
    "modela = Sequential()\n",
    "modela.add(Merge([model1, model2], mode='concat'))\n",
    "modela.add(Dense(units*2, kernel_initializer=weights, bias_initializer=bias))\n",
    "modela.add(BatchNormalization())\n",
    "modela.add(Activation('relu'))\n",
    "modela.add(Dropout(dropout))\n",
    "\n",
    "modela.add(Dense(units, kernel_initializer=weights, bias_initializer=bias))\n",
    "modela.add(BatchNormalization())\n",
    "modela.add(Activation('relu'))\n",
    "modela.add(Dropout(dropout))\n",
    "\n",
    "modelb = Sequential()\n",
    "modelb.add(Merge([model3, model4], mode='concat'))\n",
    "modelb.add(Dense(units*2, kernel_initializer=weights, bias_initializer=bias))\n",
    "modelb.add(BatchNormalization())\n",
    "modelb.add(Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "\n",
    "modelb.add(Dense(units, kernel_initializer=weights, bias_initializer=bias))\n",
    "modelb.add(BatchNormalization())\n",
    "modelb.add(Activation('relu'))\n",
    "modelb.add(Dropout(dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\legacy\\layers.py:66: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  warnings.warn('The `Merge` layer is deprecated '\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Merge([modela, modelb], mode='concat'))\n",
    "model.add(Dense(units*2, kernel_initializer=weights, bias_initializer=bias))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(units, kernel_initializer=weights, bias_initializer=bias))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(1, kernel_initializer=weights, bias_initializer=bias))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the best weights for predicting the test question pairs\n",
    "save_best_weights = 'question_pairs_weights.h5'\n",
    "\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),\n",
    "             EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')]\n",
    "history = model.fit([train_q1, train_q2, train_q1, train_q2],\n",
    "                    y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=100, #Use 100, I reduce it for Kaggle,\n",
    "                    validation_split=0.15,\n",
    "                    verbose=True,\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_stats = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                              'train_acc': history.history['acc'],\n",
    "                              'valid_acc': history.history['val_acc'],\n",
    "                              'train_loss': history.history['loss'],\n",
    "                              'valid_loss': history.history['val_loss']})\n",
    "\n",
    "summary_stats\n",
    "\n",
    "# Find the minimum validation loss during the training\n",
    "min_loss, idx = min((loss, idx) for (idx, loss) in enumerate(history.history['val_loss']))\n",
    "print('Minimum loss at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(min_loss))\n",
    "min_loss = round(min_loss, 4)\n",
    "\n",
    "# Make predictions with the best weights\n",
    "model.load_weights(save_best_weights)\n",
    "predictions = model.predict([test_q1, test_q2, test_q1, test_q2], verbose = True)\n",
    "predictions[predictions > 0.7] = 0.99\n",
    "print(\"Greater than 0.7 was {}\".format(predictions[predictions > 0.7].sum()))\n",
    "predictions[predictions < 0.3] = 0.01\n",
    "print(\"Lesser than 0.3 was {}\".format(predictions[predictions < 0.3].sum()))\n",
    "\n",
    "#Create submission\n",
    "submission = pd.DataFrame(predictions, columns=['is_duplicate'])\n",
    "submission.insert(0, 'test_id', test.test_id)\n",
    "file_name = 'submission_{}.csv'.format(min_loss)\n",
    "submission.to_csv(file_name, index=False)\n",
    "\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-05-08 12:44:13.997543'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "str(datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
